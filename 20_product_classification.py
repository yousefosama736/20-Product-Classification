# -*- coding: utf-8 -*-
"""20 Product Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WVr59QUD3ef0HtLrLRpaCCJFViC-nc3e

# classification using InceptionV3

## Getting the environment ready and the data ready

### installing necessary  dependencies and getting the data from the drive
"""

!pip install gdown
!pip install rarfile patool
!pip install tensorflow
!pip install tflearn
!pip install tensorflow matplotlib

import random
import numpy as np
import time
import gdown
import rarfile
import zipfile
import os
import shutil
import cv2
import tensorflow as tf
import numpy as np
import pickle
import matplotlib.pyplot as plt
import random
import pdb
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from tensorflow.keras.applications import VGG16, ResNet50, Xception
from keras.preprocessing import image
from tensorflow.keras import models, layers, optimizers
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from random import shuffle
from tqdm import tqdm
from google.colab import drive
from pprint import pprint

"""Getting data from Drive"""

# Mount Google Drive
drive.mount('/content/drive')

# Destination folder in Google Drive
destination_folder = '/content/drive/MyDrive/CV_Part1/Data.rar'

# Check if the data folder exists in Google Drive
data_exists = os.path.exists(destination_folder)

if not data_exists:
  # https://drive.google.com/file/d/1qTaNr8KNbFtA9H3SzLJXpi3fZ_hackwW/view
  file_id = '1qTaNr8KNbFtA9H3SzLJXpi3fZ_hackwW'
  output_file = 'Data.rar'

  gdown.download(f'https://drive.google.com/uc?id={file_id}', output_file, quiet=False)


  with zipfile.ZipFile("Data.rar", 'r') as zip_ref:
    zip_ref.extractall("Data/")

  # Source folder in Colab
  source_folder = '/content/Data.rar'
  test_source_folder = '/content/test.rar'

  # Destination folder in Google Drive
  destination_folder = '/content/drive/MyDrive/CV_Part1/'

  # Copy the folder
  shutil.copy(source_folder, destination_folder + 'Data.rar')
  shutil.copy(test_source_folder, destination_folder + 'test.rar')
else:
  # Source folder in Google Drive
  source_folder = '/content/drive/MyDrive/CV_Part1/Data.rar'
  test_source_folder = '/content/drive/MyDrive/CV_Part1/test.rar'


  # Destination folder in Colab
  destination_folder = '/content/'

  # Copy the folder
  shutil.copy(source_folder, destination_folder)
  shutil.copy(test_source_folder, destination_folder)

with zipfile.ZipFile("Data.rar", 'r') as zip_ref:
    zip_ref.extractall("Data/")
with zipfile.ZipFile("test.rar", 'r') as zip_ref:
    zip_ref.extractall("test/")

"""### moving the data around so we have a train folder with it's class and a validation folder with it's classes

so it will be easier to deal with
"""

data_dir = 'Data'
i = 1
Train_image_paths = []
Validation_image_paths = []
t = []
L = ("Train","Validation")
for j in L:
  for i in range(1, 21):
      dir = os.path.join(data_dir, 'Product Classification', str(i), j)
      image_path_class = []
      for filename in os.listdir(dir):
            image_path = os.path.join(dir, filename)
            if j == "Train":
              t.append(image_path)
            image_path_class.append(image_path)
      if j == "Train":
        Train_image_paths.append(image_path_class)
      else:
        Validation_image_paths.append(image_path_class)

# train and validation
os.makedirs("Data/Product Classification/Train", exist_ok=True)
os.makedirs("Data/Product Classification/Validation", exist_ok=True)

for i in range(len(Train_image_paths)):
  os.makedirs("Data/Product Classification/Train/" + str(i + 1), exist_ok=True)
  for j in range(len(Train_image_paths[i])):
    shutil.copy(Train_image_paths[i][j],"Data/Product Classification/Train/" + str(i + 1))

for i in range(len(Validation_image_paths)):
  os.makedirs("Data/Product Classification/Validation/" + str(i + 1), exist_ok=True)
  for j in range(len(Validation_image_paths[i])):
    shutil.copy(Validation_image_paths[i][j],"Data/Product Classification/Validation/" + str(i + 1))

"""This cell combines the train and the validation data, so if we want to train the model in the end on both of them.

Note: in this case we will use a set of data that we got from the internet as the validation data so it won't overfit
"""

# train ONLY
os.makedirs("Data/Product Classification/Train", exist_ok=True)

for i in range(len(Train_image_paths)):
  os.makedirs("Data/Product Classification/Train/" + str(i + 1), exist_ok=True)
  for j in range(len(Train_image_paths[i])):
    shutil.copy(Train_image_paths[i][j],"Data/Product Classification/Train/" + str(i + 1))

for i in range(len(Validation_image_paths)):
  os.makedirs("Data/Product Classification/Train/" + str(i + 1), exist_ok=True)
  for j in range(len(Validation_image_paths[i])):
    shutil.copy(Validation_image_paths[i][j],"Data/Product Classification/Train/" + str(i + 1))

"""### A Test function that takes a model and a test genertator from ImageDataGenerator and prints the accuracy"""

def test(model_,test_generator_):
  predictions = model_.predict(test_generator_)

  predicted_class_indices = np.argmax(predictions, axis=1)

  true_class_indices = test_generator_.classes

  # Compare predicted and true class indices to calculate accuracy
  correct_predictions = np.sum(predicted_class_indices == true_class_indices)
  total_examples = len(true_class_indices)
  accuracy = correct_predictions / total_examples
  test_loss, test_accuracy = model_.evaluate(test_generator_)
  print("test_acc_evaluate: {}".format(test_accuracy))
  print("Test Accuracy Predict: ", accuracy)

"""### The next function returns three generator train_generator, valid_generator and test_generator"""

def Data_Generators(input_shape=(299,299,3),batch_size=32,train_data_dir="Data/Product Classification/Train",valid_data_dir = "Data/Product Classification/Validation",test_dir = "test/",seed_value=42):

  np.random.seed(seed_value)
  random.seed(seed_value)
  tf.random.set_seed(seed_value)

  train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=20,          # Rotate the image by a random angle within the specified range
      width_shift_range=0.2,      # Shift the width of the image by a fraction of total width
      height_shift_range=0.2,     # Shift the height of the image by a fraction of total height
      shear_range=0.2,            # Shear intensity (shear angle in radians)
      zoom_range=0.2,             # Randomly zoom into the image
      horizontal_flip=True,       # Randomly flip the image horizontally
      brightness_range=[0.8, 1.2] # Randomly adjust brightness (1.0 represents no change)
  )

  train_generator = train_datagen.flow_from_directory(
      train_data_dir,
      target_size=input_shape[:2],
      batch_size=batch_size,
      class_mode='categorical',
      shuffle=True,
      seed=seed_value
  )

  # Create a data generator for validation
  valid_datagen = ImageDataGenerator(rescale=1./255)

  valid_generator = valid_datagen.flow_from_directory(
      valid_data_dir,
      target_size=input_shape[:2],
      batch_size=batch_size,
      class_mode='categorical',
      shuffle=False,
      seed=seed_value
  )
  test_datagen = ImageDataGenerator(rescale=1./255)

  test_generator = test_datagen.flow_from_directory(
      test_dir,
      target_size=input_shape[:2],
      batch_size=1,
      class_mode='categorical',
      seed=seed_value,
      shuffle=False  # Ensure that the order of predictions matches the order of files
  )

  return train_generator,valid_generator,test_generator

"""Getting the data Generators"""

input_shape = (299,299,3)
train_generator,valid_generator,test_generator = Data_Generators(input_shape=input_shape)

"""### getting the pretrained model InceptionV3 and leaving 100 layers at the end trainable to fine tune the model"""

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)

for layer in range(len(base_model.layers)-100):
    base_model.layers[layer].trainable = False

model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(20, activation='softmax')  # 20 output classes
])
# Compile the model
model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

"""### Training the model and saving the model with the best validation accuracy

"""

# model = load_model('/content/drive/MyDrive/CV_Part1/checkPointsCV/train100_val91_test72.5.h5')

checkpoint_path = '/content/drive/MyDrive/CV_Part1/checkPointsCV/checkPointHighestVal.h5'
model_checkpoint = ModelCheckpoint(
    filepath=checkpoint_path,
    save_best_only=True,  # Only save the model if it improves on the current best
    monitor='val_accuracy',  # Monitor validation accuracy
    mode='max',  # 'max' means the training will save the model when validation accuracy increases
    save_weights_only=False,  # Save the entire model, including architecture and optimizer
    verbose=1,  # Display messages about the saving process
    save_freq='epoch'
)
t = time.time()
# Train the model
model.fit(train_generator,
          steps_per_epoch=train_generator.samples // 30,
          epochs=30,
          validation_data=valid_generator,
          shuffle=True,
          callbacks=[model_checkpoint])
print(f"Train done in {int(time.time()-t)/60} min")
model.save('/content/drive/MyDrive/CV_Part1/checkPointsCV/final_test_model.h5')

model = load_model('/content/drive/MyDrive/CV_Part1/checkPointsCV/train96_val97_test78.6.h5')

test(model,test_generator)

"""## from here will be training the model on the training dataset and the validation dataset together"""

train_generator,valid_generator,test_generator = Data_Generators(input_shape=input_shape,valid_data_dir="test/")

"""### load the best model that got trained on the train dataset only"""

model = load_model('/content/drive/MyDrive/CV_Part1/checkPointsCV/train96_val97_test78.6.h5')

checkpoint_path = '/content/drive/MyDrive/CV_Part1/checkPointsCV/checkPointHighestVal.h5'
model_checkpoint = ModelCheckpoint(
    filepath=checkpoint_path,
    save_best_only=True,  # Only save the model if it improves on the current best
    monitor='val_accuracy',  # Monitor validation accuracy
    mode='max',  # 'max' means the training will save the model when validation accuracy increases
    save_weights_only=False,  # Save the entire model, including architecture and optimizer
    verbose=1,  # Display messages about the saving process
    save_freq='epoch'
)
t = time.time()
# Train the model
model.fit(train_generator,
          steps_per_epoch=train_generator.samples // 29,
          epochs=15,
          validation_data=valid_generator,
          shuffle=True,
          callbacks=[model_checkpoint])
print(f"Train done in {int(time.time()-t)/60} min")
model.save('/content/drive/MyDrive/CV_Part1/checkPointsCV/final_test_model.h5')

model = load_model('/content/drive/MyDrive/CV_Part1/checkPointsCV/train_val99_test80.h5')

test(model,test_generator)

"""## Test script

if there was missing classes create it with empty folder
"""

# Set of classes you trained your model on
trained_classes = []
for i in range(1,21):
  trained_classes.append(str(i))
trained_classes = set(trained_classes)
print(trained_classes)
# Path to the folder containing classes
folder_path = 'test_path/'  # change this path to the data you want to test on

# Get the list of subdirectories (classes) in the folder
current_classes = set(os.listdir(folder_path))
print(current_classes)
# Find missing classes
missing_classes = trained_classes - current_classes
print(missing_classes)
# Create missing classes
for missing_class in missing_classes:
    class_path = os.path.join(folder_path, missing_class)
    os.makedirs(class_path)
    print(f"Created missing class: {missing_class}")

"""Creating a data generator for the test data"""

input_shape = (299, 299, 3)
test_script_dir = "test_path/"
test_script_datagen = ImageDataGenerator(rescale=1./255)

test_script_generator = test_script_datagen.flow_from_directory(
    test_script_dir,
    target_size=input_shape[:2],
    batch_size=1,
    class_mode='categorical',
    # classes = ["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20"],
    seed=42,
    shuffle=False  # Ensure that the order of predictions matches the order of files
)

"""load the model"""

loaded_model = load_model('/content/drive/MyDrive/CV_Part1/checkPointsCV/train_val97_test86.h5')

def test(model_,test_generator_):
  predictions = model_.predict(test_generator_)

  predicted_class_indices = np.argmax(predictions, axis=1)

  true_class_indices = test_generator_.classes

  # Compare predicted and true class indices to calculate accuracy
  correct_predictions = np.sum(predicted_class_indices == true_class_indices)
  total_examples = len(true_class_indices)
  accuracy = correct_predictions / total_examples
  test_loss, test_accuracy = model_.evaluate(test_generator_)
  print("test_acc_evaluate: {}".format(test_accuracy))
  print("Test Accuracy Predict: ", accuracy)

test(loaded_model,test_script_generator)